{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zara_es_scrapper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMw4Ri2Qba1Q"
      },
      "source": [
        "## LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsjzoUbPbdsD"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from io import BytesIO\n",
        "import gzip\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jkY_6NLbk7Y"
      },
      "source": [
        "## SETTINGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lKRKdttbiAR"
      },
      "source": [
        "url = 'https://www.zara.com/sitemaps/sitemap-index.xml.gz'\n",
        "country = 'sitemap-es-es'\n",
        "gender = 'mujer'\n",
        "item_types = ['vestido'] \n",
        "keyword_script = 'detailedComposition'\n",
        "output = 60 # replace with len(item_urls)-1 if you want to extract all the items of the category\n",
        "dir = 'data'\n",
        "file_name_items = 'itemsDimension.csv'\n",
        "file_name_comp = 'compDimension.csv'\n",
        "code = 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FL-NrOrccfH"
      },
      "source": [
        "## FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIwnsB4gcelp"
      },
      "source": [
        "## REQUESTS - SOUP\n",
        "\n",
        "def decompressRequestSoup(url):\n",
        "    \"\"\"Given an url perform a get request,\n",
        "    decompress content and inject in soup\n",
        "    object.\n",
        "\n",
        "    Args:\n",
        "    :param url: an url wich returnes compressed\n",
        "    content\n",
        "\n",
        "    Return: soup object of the response contents\n",
        "\n",
        "    \"\"\"\n",
        "    r_comp = requests.get(url)\n",
        "    decomp = gzip.GzipFile(fileobj=BytesIO(r_comp.content))\n",
        "    soup = BeautifulSoup(decomp.read(), features=\"html.parser\")\n",
        "    return soup\n",
        "\n",
        "\n",
        "\n",
        "def requestSoup(url):\n",
        "    \"\"\"Given an url perform a get request and\n",
        "    set response in a soup object.\n",
        "\n",
        "    Args:\n",
        "    :param url: an url to soup, string.\n",
        "\n",
        "    Return: soup object of get response\n",
        "\n",
        "    \"\"\"\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, features=\"html.parser\")\n",
        "    return soup\n",
        "\n",
        "\n",
        "## ZARA SITE NAVIGATION\n",
        "\n",
        "def getGeneralLinks(url, country, gender):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\"Get all general clothing urls from web sitemap\n",
        "\n",
        "    Args:\n",
        "    :param url: sitemap, string. \n",
        "    :param country: pattern string.\n",
        "    :param gender: pattern string, language depends on\n",
        "    country pattern.\n",
        "    \n",
        "    Return: list of general clothing types urls\n",
        "\n",
        "    \"\"\"\n",
        "    soup = decompressRequestSoup(url)\n",
        "    links = soup.findAll('loc')\n",
        "    for link in links:\n",
        "        href = link.text\n",
        "        if re.search(country, href) != None:\n",
        "            time_1 = time.time()\n",
        "            es_url = href\n",
        "            es_soup = decompressRequestSoup(es_url)\n",
        "            es_links = es_soup.findAll('loc')\n",
        "            es_links_mujer = [link.text for link in es_links\n",
        "                              if re.search(gender, link.text) != None]\n",
        "            time_2 = time.time()\n",
        "            resp_time = time_2 - time_1\n",
        "            time.sleep(resp_time+1)\n",
        "\n",
        "    return es_links_mujer\n",
        "\n",
        "\n",
        "def selectGeneralType(url, country, gender, item_types):\n",
        "    \"\"\"Get general types of clothing filtered url list from\n",
        "    web sitemap.\n",
        "\n",
        "    Args:\n",
        "    ;url: (sitemap)\n",
        "    ;country: (pattern string)\n",
        "    ;gender: (pattern string, language depends on\n",
        "    coutry pattern)\n",
        "    ;item_types: (list of types of clothing to\n",
        "    filter).\n",
        "\n",
        "    Returns: list of general clothing sections urls.\n",
        "\n",
        "    \"\"\"\n",
        "    link_list = getGeneralLinks(url, country, gender)\n",
        "    pattern = '|'.join(item_types)\n",
        "    type_links = [link for link in link_list\n",
        "                  if re.search(pattern, link) != None]\n",
        "    return type_links\n",
        "\n",
        "\n",
        "\n",
        "def getItemUrl(url, country, gender, item_types):\n",
        "    \"\"\"Given a list of general cloth sections it\n",
        "     gets the url of each item in each.\n",
        "\n",
        "    Args:\n",
        "    :param url: (sitemap)\n",
        "    :param country: (pattern string)\n",
        "    :param gender: (pattern string, language depends on\n",
        "    :param coutry pattern)\n",
        "    :param item_types: (list of types of clothing to\n",
        "    filter).\n",
        "\n",
        "    Retturns: list of items urls.\n",
        "\n",
        "    \"\"\"\n",
        "    gen_urls = selectGeneralType(url, country, gender, item_types)\n",
        "    for gen_url in gen_urls:\n",
        "        time_1 = time.time()\n",
        "        gen_soup = requestSoup(gen_url)\n",
        "        #gen_soup\n",
        "        time_2 = time.time()\n",
        "        resp_time = time_2 - time_1\n",
        "        item_cont = gen_soup.find_all('a', class_=\"name _item\")\n",
        "        item_urls = [cont.get('href') + '#' for cont in item_cont]  # adding hash to access app content page\n",
        "        time.sleep(resp_time + 2)\n",
        "\n",
        "        return item_urls\n",
        "\n",
        "\n",
        "## DATAFRAME FILLING\n",
        "\n",
        "def fillDataFrames(url, country, gender, item_types, keyword_script, output, code, df_items, df_comp):\n",
        "    \"\"\"Given the list of items urls, extract key features (price, composition,\n",
        "    description) and build a dataset\n",
        "    :param url: (sitemap)\n",
        "    :param country: (pattern string)\n",
        "    :param gender: (pattern string, language depends on\n",
        "    :param coutry pattern)\n",
        "    :param item_types: (list of types of clothing to\n",
        "    filter).\n",
        "    :param keyword_script: string to match right script\n",
        "    :param output: limit of items that we want to get\n",
        "    :param code: integer we'll use to add to our index to create item_codes\n",
        "    :param df_items: name of items dataframe that we defined\n",
        "    :param df_comp: name of composition dataframe that we defined\n",
        "    :return: a dataset with key informaition of every item (price, composition,\n",
        "    description)\n",
        "    \"\"\"\n",
        "    df_items = pd.DataFrame(columns=['item_code',\n",
        "                                     'item_name',\n",
        "                                     'item_desc',\n",
        "                                     # 'item_composition_ext',\n",
        "                                     # 'item_composition_int',\n",
        "                                     'item_price'])\n",
        "    item_urls = getItemUrl(url, country, gender, item_types)\n",
        "    \n",
        "    i = 0\n",
        "    while i < output:\n",
        "        time_1 = time.time()\n",
        "        item_url = item_urls[i]\n",
        "        item_code = code + i\n",
        "        i += 1\n",
        "        item_script_all = requestSoup(item_url).find_all('script',\n",
        "                                                            type=\"text/javascript\") \n",
        "        time_2 = time.time()\n",
        "        resp_time = time_2 - time_1\n",
        "        item_script = [scrpt.text for scrpt in item_script_all if re.search(keyword_script, str(scrpt)) != None]\n",
        "        time.sleep(resp_time + 7)\n",
        "        for item_app in item_script:\n",
        "            item_info = item_app[item_app.find(';window.zara.dataLayer ='):].replace(\n",
        "                    ';window.zara.viewPayload = window.zara.dataLayer;', '').replace(';window.zara.dataLayer =', '').replace('á','a').replace(\n",
        "                        'é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "            try:\n",
        "                parsed = json.loads(item_info)\n",
        "\n",
        "                # Dataset items\n",
        "                ##name\n",
        "                name = parsed['product']['name']\n",
        "\n",
        "                ## Descripció\n",
        "                desc = '\"'+parsed['product']['detail']['rawDescription']+'\"'\n",
        "                #print(desc)\n",
        "\n",
        "                ## Price\n",
        "                price = parsed['product']['detail']['colors'][0]['price']\n",
        "                \n",
        "                # Join Life\n",
        "                try:\n",
        "                  joinlife_title = parsed['product']['detail']['extraInfo']['joinLifeExtraInfo']['title']\n",
        "                  joinlife_desc = '\"'+parsed['product']['detail']['extraInfo']['joinLifeExtraInfo']['description']+'\"'\n",
        "                  join_life = True\n",
        "                except KeyError:\n",
        "                  joinlife_title = \"\"\n",
        "                  joinlife_desc = \"\"\n",
        "                  join_life = False\n",
        "\n",
        "                new_row = {'item_code': item_code,\n",
        "                            'item_name': name,\n",
        "                            'item_desc': desc.replace('\\n',' '),\n",
        "                            'join_life': join_life,\n",
        "                            'joinlife_title': joinlife_title.replace('\\n',' '),\n",
        "                            'joinlife_desc':joinlife_desc.replace('\\n',' '),\n",
        "                            'item_price': price}\n",
        "\n",
        "                # append row to the dataframe\n",
        "                df_items = df_items.append(new_row, ignore_index=True)\n",
        "                \n",
        "                # Dataset composition\n",
        "                parts = parsed['product']['detail']['detailedComposition']['parts']\n",
        "                for ele in parts:\n",
        "                  part_name = ele['description']\n",
        "                  compo = ele['components']\n",
        "\n",
        "                  for mat in compo:\n",
        "                    # material name\n",
        "                    material = mat['material']\n",
        "                    \n",
        "                    # material percent\n",
        "                    percent = mat['percentage']\n",
        "\n",
        "                    new_row_comp = {'item_code': item_code,\n",
        "                                        'part_name':part_name,\n",
        "                                        'material':material,\n",
        "                                        'percent': percent}\n",
        "\n",
        "                    # append row to the dataframe\n",
        "                    df_comp = df_comp.append(new_row_comp, ignore_index=True)\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "\n",
        "## STORAGE\n",
        "\n",
        "def storeCSV(df,file_name):\n",
        "      \"\"\"Given a dataframe and a file name \n",
        "      store the dataframe in as a csv with \n",
        "      pipe (|) as separation and no\n",
        "      index.\n",
        "\n",
        "      :param df: dataset to store\n",
        "      :param file_name: name of the file we want to create\n",
        "\n",
        "      \"\"\"\n",
        "      df.to_csv(file_name,encoding='utf-8', sep='|',index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxbaEasOdIkq"
      },
      "source": [
        "## MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fFak-6cdKxs"
      },
      "source": [
        "# 1 - Initialize items and composition datasets. If you want to \n",
        "#     fill them gradually, comment these definition:\n",
        "\n",
        "df_items = pd.DataFrame(columns=['item_code',\n",
        "                                 'item_name',\n",
        "                                 'item_desc',\n",
        "                                 'join_life',\n",
        "                                 'joinlife_title',\n",
        "                                 'joinlife_desc',\n",
        "                                 'item_price'])\n",
        "\n",
        "df_comp = pd.DataFrame(columns=['item_code',\n",
        "                                'part_name',\n",
        "                                'material',\n",
        "                                'percent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OJRBmI_dOBS"
      },
      "source": [
        "# 2 - Execute function to fill the dataframes:\n",
        "\n",
        "fillDataframes(url, country, gender, item_types, keyword_script, output, code, \n",
        "               df_items, df_comp):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH1gyNKFdcAe"
      },
      "source": [
        "# 3 - Execute storeCSV function for each of the datasets to get your\n",
        "#     input saved in a CSV file separated by pipe symbol (|)\n",
        "\n",
        "storeCSV(df_items,file_name_items)\n",
        "storeCSV(df_comp,file_name_comp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
